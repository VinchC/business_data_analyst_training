## Introduction au Text mining - Pr√©traitement

Apr√®s avoir saisi les expressions r√©guli√®res, l'objectif de cet exercice est de pr√©senter les outils fondamentaux du text mining, avec notamment : la tokenisation, la technique Bag of words, le filtrage Stop words, la racinisation et la lemmatisation. Ce sont des techniques tr√®s utilis√©es pour les travaux pr√©liminaires n√©cessaires √† toute analyse se basant sur des donn√©es de type texte.

# La tokenization

La tokenization est une t√¢che tr√®s courante en linguistique, celle-ci consiste √† segmenter des textes en unit√©s plus petites. Par exemple une phrase est "tokeniz√©e" en mot ou un paragraphe est tokeniz√© en phrase. Le d√©coupage le plus courant est celui dans lequel les unit√©s de base sont des tokens (mots, chiffres ou ponctuations) et pour un m√™me texte il existe plusieurs tokenizations possibles.

Une tokenisation possible de la phrase suivante : "La science des donn√©es est l'extraction de connaissances." sera alors : ['la', 'science', 'des', 'donn√©es', 'est', 'l'extraction', 'de', 'connaissances'].

Le package Scikit-learn et la bo√Æte-√†-outil : Natural Language Toolkit (NLTK) sont deux biblioth√®ques qui vont permettre de cr√©er des programmes pour l'analyse de texte.

Assigner √† la variable txt le c√©l√®bre couplet de Moli√®re suivant :
"Souffrez qu'Amour cette nuit vous r√©veille. Par mes soupirs laissez-vous enflammer. Vous dormez trop, adorable merveille. Car c'est dormir que de ne point aimer."

Importer la classe PunktSentenceTokenizer du package nltk.tokenize.

Initialiser tokenizer, une instance de la classe PunktSentenceTokenizer.

√Ä l'aide de la m√©thode tokenize de l'objet tokenizer, d√©couper txt en tokens. .

Rappelez-vous qu'avant de regarder la solution, vous avez toujours acc√®s √† l'aide officielle de Python en tapant help(nom_fonction) dans la console.

### Ins√©rez votre code ici

‚Äã
‚Äã
‚Äã
La technique de tokenisation impl√©ment√©e ci-dessus, consiste √† d√©couper un paragraphe en plusieurs phrases. Selon les situations il peut √™tre √©galement int√©ressant de d√©couper les phrases en mots. Le sous package nltk.tokenize impl√©mente la fonction word_tokenize permettant d'effectuer le d√©coupage des donn√©es textes en mots.

Importer la fonction word_tokenize du sous package nltk.tokenize.
Appliquer la m√©thode word_tokenize √† la phrase : txt, en pr√©cisant que le langage utilis√© est le fran√ßais (french).
Stocker les tokens dans la variable mots.
‚ÄÉ La m√©thode word_tokenize s'utilise comme suit : mots = word_tokenize(phrase, language = 'french'). Elle permet de d√©couper le paragraphe txt en une liste de mots.
‚ÄÉ Si une erreur apparait concernant le t√©l√©chargement des packages utilis√©s dans ce notebook, veuillez effectuer la commande suivante :  
 import nltk
nltk.download('package_name')

### Ins√©rez votre code ici

‚Äã
‚Äã

# Le filtrage Stop Words

Apr√®s avoir d√©coup√© la phrase en mots, nous pouvons remarquer que la variable mots contient des mots dont certains sont tr√®s fr√©quemment rencontr√©s, il s'agit de "mots vides". Un mot vide (ou stop word, en anglais) est un mot qui est tellement commun qu'il est inutile de l'indexer ou de l'utiliser dans une recherche. En fran√ßais, des mots vides √©vidents pourraient √™tre : "le", "la", "de", "du", "ce", ...

La librairie NLTK contient quelques mots vides, pour les conna√Ætre :

Ex√©cuter la cellule de code ci-dessous.

# Importer stopwords de la classe nltk.corpus

from nltk.corpus import stopwords
‚Äã

# Initialiser la variable des mots vides

stop_words = set(stopwords.words('french'))
print(stop_words)
‚Äã
La variable stop_words est de type set. Il est possible d'ajouter un mot vide au set stop_words en ex√©cutant la commande suivante : stop_words.add("mot_vide").

Et pour ajouter plusieurs mots vides en utilisant la m√©thode update comme suit : stop_words.update([word1, word2]).

Ajouter au set stop_words les deux mots vides suivants : "." et ",".

D√©finir la fonction stop_words_filtering qui permet de supprimer les mots vides d'une liste pass√©e en param√®tre.

Mettre √† jour la liste mots en supprimant les mots vides du set stop_words.

### Ins√©rez votre code ici

‚Äã
‚Äã
Les mots vides ne sont plus pr√©sents dans le corpus : "vous", "mes", "que", "de", "ne", "," et ".". Cependant, nous pouvons toujours remarquer que les tokens contiennent toujours des mots compos√©s : "laissez-vous" et "c'est". Ainsi, la biblioth√®que NLTK offre la possibilit√© de d√©couper un corpus en gardant que des tokens particuliers par le biais des expressions r√©guli√®res.

Importer la classe RegexpTokenizer du package nltk.tokenize.regexp.

Initialiser un tokeniseur, tokenizer, √† l'aide de la fonction RegexpTokenizer.

Convertir les cha√Ænes de caract√®res, √† l'aide de la m√©thode tokenize de l'objet tokenizer, en ne conservant que les mots contenant plus de quatre caract√®res.

Affecter la sortie de la m√©thode RegexpTokenizer √† la variable tokens. Penser √©galement √† convertir la phrase txt en minuscules en utilisant la m√©thode lower.

### Ins√©rez votre code ici

‚Äã
‚Äã
L'avantage de cette m√©thode est qu'elle permet d'appliquer une tokenisation personnalis√©e. Il est possible de cibler des tokens particuliers lors de la fouille de textes. En pratique, la tokenization et le filtrage stop words est une √©tape pr√©liminaire √† la vectorisation Bag of Words qui est un algorithme de "vectorisation" des mots.

# Algorithme Bag of Words

1. CountVectorizer
   L'algorithme Bag of Words consiste √† "vectoriser" un document. La repr√©sentation en sac de mot consiste √† repr√©senter un document par le nombre d'occurrences des mots qu'il contient. Cette m√©thode "naive" de repr√©sentation de document peut √™tre impl√©ment√©e par la classe CountVectorizer du package sklearn.feature_extraction.text qui peut produire une repr√©sentation en sac de mots d'une cha√Æne ou d'un fichier.

Cet algorithme est tr√®s utilis√© en pratique car il est simple √† impl√©menter et donne des r√©sultats satisfaisants malgr√© sa repr√©sentation "pauvre" en signification. De nos jours, la vectorisation de texte est plus sophistiqu√©e et se fait √† l'aide des r√©seaux de neurones r√©currents (deep learning).

Pour vous aider √† retenir l'essentiel, en voici un aper√ßu d'utilisation :

#Initialiser un vectorisateur vectorizer
vectorizer = CountVectorizer()

#Appliquer l'algorithme de num√©rotation
tokens = vectorizer.fit_transform(string)

#R√©cup√©rer les _tokens_ num√©rot√©s</code>
vectorizer.vocabulary\_

#Pour afficher la repr√©sentation vectorielle d'une nouvelle phrase
vectorizer.transform(["une nouvelle phrase"]).toarray()

<img src="pictures/CountVectorizer.png">

Importer la classe CountVectorizer du package sklearn.feature_extraction.text.

Initialiser vectorizer, en utilisant la m√©thode CountVectorizer.

Convertir les cha√Ænes de caract√®res, √† l'aide de la m√©thode fit_transform de l'objet vectorizer, en tokens.

R√©cup√©rer les tokens num√©rot√©s.

### Ins√©rez votre code ici

‚Äã
‚Äã
‚Äã
Cette repr√©sentation vectorielle peut s'apparenter √† un tableau dont les num√©ros affich√©s sont les indices de chaque mot :

<img src="pictures/vector.png">

Afficher la repr√©sentation vectorielle de la phrase "laissez-vous enflammer" et "dormez vous cette nuit ?"

### Ins√©rez votre code ici

‚Äã
‚Äã
La premi√®re liste correspond √† l'application du vecteur vectorizer sur la premi√®re phrase : "laissez-vous enflammer".

En reprennant le tableau de notre vectorisation, on observe que notre liste contient un 1 lorsque l'un des mot de la phrase est bien pr√©sent dans notre tableau, et 0 si ce n'est pas le cas. En effet, le 7√®me √©l√©ment de la liste vaut 1 car la phrase comporte le mot "laissez", pr√©sent dans le tableau √† l'indice 7. On obtient ainsi la fr√©quence de chaque mot de notre phrase sur leur indice dans la liste.

La deuxi√®me liste effectue la m√™me manipulation avec la deuxi√®me phrase : "Dormez vous cette nuit ?"

Executer la cellule suivante
print(vectorizer.transform(["Dormez vous vous cette nuit ?"]).toarray())
Ce dernier exemple illustre bien l'obtention de la fr√©quence d'un mot √† l'aide de cette m√©thode.

2. TF-IDF
   Alors que CountVectorizer se concentre uniquement sur la fr√©quence brute des mots, le score TF-IDF introduit une notion de pertinence en attribuant des poids qui tiennent compte de la fr√©quence dans le document et de la raret√© dans l'ensemble du corpus.

Le TF-IDF peut se d√©finir comme le produit de deux scores mesurant l'importance d'un mot d'un texte :

TF-IDF(ùëñ,ùëó) = TF(ùëñ,ùëó) √ó IDF(ùëñ)
avec

- TF (Term Frequency), le score repr√©sentant la fr√©quence d'apparition d'un mot dans une phrase.
- IDF (Inverse Document Frequency), le score repr√©sentant la sp√©cificit√© d'un mot dans un corpus de textes.

Cette m√©thode est impl√©ment√©e par la classe TfidfVectorizer du package sklearn.feature_extraction.text permet d'impl√©menter la formule ci_dessus.

Voici un aper√ßu d'utilisation :

#Initialiser un vectorisateur vectorizer
vectorizer = TfidfVectorizer()

#Appliquer l'algorithme de num√©rotation
tokens = vectorizer.fit_transform(string)

#R√©cup√©rer les _tokens_ num√©rot√©s</code>
vectorizer.vocabulary\_

#Pour afficher la repr√©sentation vectorielle d'une nouvelle phrase
vectorizer.transform(["une nouvelle phrase"]).toarray()

<img src="pictures/TF_IDF.png">

- Importer la classe TfidfVectorizer du package sklearn.feature_extraction.text.
- Initialiser vectorizer_tfidf, en utilisant la m√©thode TfidfVectorizer.
- Convertir les cha√Ænes de caract√®res, √† l'aide de la m√©thode fit_transform de l'objet vectorizer_tfidf, en tokens.
- R√©cup√©rer les tokens num√©rot√©s.

### Ins√©rez votre code ici

‚Äã
‚Äã
Cette repr√©sentation vectorielle est la m√™me que celle effectu√©e pr√©c√©demment avec le CountVectorizer :

<img src="pictures/vector.png">

Afficher la repr√©sentation vectorielle de la phrase "laissez-vous enflammer" et "dormez vous cette nuit ?"

### Ins√©rez votre code ici

‚Äã
‚Äã
De la m√™me fa√ßon que pour la vectorisation avec CountVectorizer, les listes comportent des num√©ros lorsque qu'un mot de chaque phrase est bien pr√©sent dans notre tableau. En plus de rep√©rer la pr√©sence de chaque mot, les valeurs que l'on peut voir dans la liste correspondent au score de sp√©cificit√© du mot (le param√®tre IDF). Plus ce score est haut et plus le mot est sp√©cifique √† cette phrase au vue du corpus de texte, c'est √† dire qu'il est peut courant dans le corpus et a donc une plus grande importance dans la phrase.

Limitations du Bag of Words : L'ordre des mots n'est pas pris en compte dans la repr√©sentation Bag of Words, ainsi cette repr√©sentation ne peut √™tre utilis√©e pour des approches de type syntaxiques.

Afin de finir notre pr√©traitement textuel, il est d'usage de normaliser les mots obtenus, afin de les regrouper pour les rendre plus facilement comparables ou analysables. On utilise ainsi deux techniques de pr√©traitement de donn√©es textuelles :

# Normalisation lexicale

1. La racinisation
   La racinisation ou la d√©suffixation (stemming en anglais) est une technique de transformation des mots en leur radical ou racine (stem en anglais). La racine d'un mot correspond √† la partie du mot restante une fois que l‚Äôon a supprim√© ses pr√©fixes et suffixes. Ainsi, il est parfois pertinent de rassembler tous les mots d'une m√™me famille autour de la racine pour des analyses de textes.

La biblioth√®que NLTK impl√©mente l'algorithme de stemming. Pour vous aider √† retenir l'essentiel, en voici un aper√ßu d'utilisation :

# Initialiser un racinisateur porter_stemmer

porter_stemmer = FrenchStemmer()

# Calculer la racine du mot **word**

racine = porter_stemmer.stem(word)

- Importer la classe FrenchStemmer du package nltk.stem.snowball.
- Initialiser un objet stemmer en utilisant l'application FrenchStemmer.
- Retrouver la racine du mot : s√©rieusement.

### Ins√©rez votre code ici

‚Äã
‚Äã
‚Äã
L'analyse de sentiment est une application qui d√©coule des m√©thodes de text mining. Cela consiste √† d√©finir les opinions, sentiments ou attitudes pr√©sents dans un texte ou un ensemble de texte. La racine des mots contiennent souvent l'opinion et le sentiment d√©gag√©, c'est la raison pour laquelle le plus souvent les analyses de sentiments sont men√©es √† l'aide d'algorithmes de stemming.

- D√©finir une fonction stemming qui retrouve la racine pour chaque mot de mots, une liste (de mots) pass√©e en param√®tre.
- Faire en sorte que la fonction ne renvoie aucun doublon.
- Appliquer la fonction stemming √† la variable tokens.

### Ins√©rez votre code ici

‚Äã
‚Äã
‚Äã 2. La lemmatisation
La lemmatisation (lemmatisation en anglais) est une technique similaire √† la racinisation mais plus avanc√©e. Elle consiste √† transformer des mots en leur lemme (lemma en anglais), c'est-√†-dire en un terme issu de l‚Äôusage ordinaire des locuteurs de la langue (forme dans le dictionnaire). Contrairement √† la racinisation, la lemmatisation tient compte du sens s√©mantique des mots et peut produire des formes qui sont des mots valides.

La biblioth√®que NLTK impl√©mente l'algortithme de lemmatisation. Pour vous aider √† retenir l'essentiel, en voici un aper√ßu d'utilisation

# Initialiser un lemmatiseur wordnet_lemmatizer

wordnet_lemmatizer = WordNetLemmatizer()

#Calcul le lemme
wordnet_lemmatizer.lemmatize('meeting', pos='v')

- Ex√©cuter la cellule de code ci-dessous, qui calcule le lemme du mot meeting en supposant qu'il s'agit d'un verbe.
- M√™me instruction en supposant qu'il s'agit d'un nom.

La m√©thode lemmatize a un deuxi√®me attribut (param√®tre) optionnel : pos. Il permet de pr√©ciser si le mot √† lemmatiser est consid√©r√© comme un verbe pos = 'v' ou un nom pos = 'n'.

### Ins√©rez votre code ici

‚Äã
‚Äã
‚Äã
La lemmatisation est g√©n√©ralement plus complexe et prend plus de temps que la racinisation en raison de sa sophistication linguistique, mais elle fournit des r√©sultats plus pr√©cis.

# Conclusion

Dans ce notebook, nous avons vu des outils tr√®s important pour le Text Mining, √† savoir :

- la tokenisation, c'est-√†-dire la segmentation d'un texte en tokens
- le filtrage des stopwords, les mots trop communs utilis√©s dans les textes (mots de liaisons, article d√©fini, etc..)
- l'algorithme Bag of Words, qui repr√©sente de mani√®re vectorielle des documents √† l'aide soit d'un CountVectorizer ou d'un TF-IDF pour calculer la fr√©quence des mots
- la normalisation lexicale, pour harmoniser et regrouper les mots avec leurs origines, que ce soit leur racine pour la racinisation, ou leur lemme pour la lemmatisation.
  L'objectif du prochain notebook est de pouvoir les appliquer pour calculer la fr√©quence des mots et de faire une repr√©sentation visuelle des mots clefs rep√©r√©s √† l'aide d'un nuage de mots. N'h√©sitez pas √† revenir √† cet exercice pour se rappeler des notions importantes de ce chapitre.
